{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging; logging.basicConfig(level=logging.INFO)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logictensornetworks as ltn\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.linewidth'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package mac_morpho to\n",
      "[nltk_data]     /home/castro/nltk_data...\n",
      "[nltk_data]   Package mac_morpho is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('mac_morpho')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Word2Vec object from words.embedding\n",
      "INFO:gensim.utils:loading wv recursively from words.embedding.wv.* with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute cum_table to None\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname': 'words.embedding', 'datetime': '2021-12-03T18:02:28.947187', 'gensim': '4.1.2', 'python': '3.8.10 (default, Sep 28 2021, 16:10:42) \\n[GCC 9.3.0]', 'platform': 'Linux-5.10.60.1-microsoft-standard-WSL2-x86_64-with-glibc2.29', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "from nltk.corpus import mac_morpho\n",
    "import glob\n",
    "path = ['./false_text/','./true_text/']\n",
    "\n",
    "model = None\n",
    "if(not os.path.isfile('words.embedding')) :\n",
    "    model = gensim.models.Word2Vec(mac_morpho.sents())\n",
    "    new_texts = []\n",
    "    for i in range(2):\n",
    "        for file_name in glob.glob(path[i]+\"*.txt\"):\n",
    "            with open(file_name, 'r') as file:  \n",
    "                text = list(gensim.utils.tokenize(file.read()))\n",
    "                new_texts.append(text)\n",
    "    model.min_count = 1\n",
    "    model.build_vocab(new_texts, update=True)\n",
    "    model.train(new_texts, total_examples=len(new_texts), epochs=model.epochs)\n",
    "    \n",
    "    model.save('words.embedding')\n",
    "else:\n",
    "    model = gensim.models.Word2Vec.load('words.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./false_text/2.txt ['Eu', 'passei', 'oito', 'anos', 'na', 'prefeitura', 'e', 'você', 'não', 'viu', 'aumento', 'de', 'IPTU', 'você', 'não', 'viu', 'aumento', 'de', 'imposto']\n",
      "./false_text/6.txt ['Nunca', 'se', 'investiu', 'tanto', 'na', 'saúde', 'quanto', 'o', 'nosso', 'governo']\n",
      "./false_text/5.txt ['Eu', 'quero', 'informar', 'a', 'você', 'que', 'nos', 'assiste', 'hoje', 'que', 'nós', 'vamos', 'recontratar', 'os', 'profissionais', 'que', 'o', 'Crivella', 'demitiu', 'e', 'recuperar', 'todas', 'as', 'clínicas', 'da', 'família']\n",
      "./false_text/4.txt ['O', 'Rodrigo', 'Bethlem', 'esse', 'que', 'te', 'assessora', 'aqui', 'no', 'intervalo', 'quando', 'ele', 'confessou', 'que', 'tomava', 'dinheiro', 'da', 'prefeitura', 'eu', 'botei', 'ele', 'pra', 'correr', 'Virou', 'seu', 'assessor']\n",
      "./false_text/1.txt ['E', 'aliás', 'quando', 'ele', 'fala', 'do', 'aumento', 'na', 'nota', 'do', 'Ideb', 'que', 'melhorou', 'não', 'foi', 'a', 'nota', 'da', 'prova', 'O', 'que', 'melhorou', 'dele', 'foi', 'a', 'aprovação', 'dos', 'alunos']\n",
      "./false_text/3.txt ['De', 'lá', 'pra', 'cá', 'a', 'população', 'de', 'rua', 'antes', 'da', 'pandemia', 'até', 'aumentou', 'mais', 'do', 'que', 'triplicou', 'mais', 'de', 'mil', 'para', 'mais', 'de', 'mil', 'pessoas']\n",
      "./false_text/8.txt ['de', 'passagem', 'de', 'ônibus', 'não', 'tem', 'em', 'lugar', 'nenhum', 'do', 'Brasil']\n",
      "./false_text/7.txt ['PSOL', 'entrou', 'no', 'STF', 'para', 'obrigar', 'as', 'prefeituras', 'a', 'colocar', 'ideologia', 'de', 'gênero', 'para', 'as', 'crianças']\n",
      "./true_text/2.txt ['O', 'seu', 'tio', 'pelo', 'que', 'eu', 'sei', 'é', 'a', 'favor', 'do', 'aborto']\n",
      "./true_text/6.txt ['R', 'bilhões', 'de', 'contas', 'para', 'pagar', 'e', 'R', 'bilhões', 'de', 'queda', 'de', 'arrecadação', 'que', 'caiu']\n",
      "./true_text/5.txt []\n",
      "./true_text/4.txt ['No', 'seu', 'governo', 'a', 'fila', 'do', 'Sisreg', 'triplicou', 'Hoje', 'são', 'mil', 'pessoas', 'esperando', 'por', 'uma', 'consulta', 'um', 'exame', 'ou', 'uma', 'cirurgia']\n",
      "./true_text/1.txt ['O', 'TCM', 'em', 'já', 'decidiu', 'sobre', 'essa', 'questão', 'dizendo', 'que', 'nós', 'deixamos', 'recursos', 'em', 'caixa', 'para', 'ele', 'seguir', 'com', 'a', 'vida']\n",
      "./true_text/3.txt ['Nós', 'tivemos', 'o', 'dobro', 'do', 'índice', 'de', 'letalidade', 'de', 'São', 'Paulo']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "qtd_words = 10\n",
    "null_word = [0]*100\n",
    "big_word_count = 0\n",
    "for i in range(2):\n",
    "    for file_name in glob.glob(path[i]+\"*.txt\"):\n",
    "        with open(file_name, 'r') as file:\n",
    "            text = list(gensim.utils.tokenize(file.read()))\n",
    "            print(file_name, text)\n",
    "            vec_words = [model.wv[word] for word in text]\n",
    "            data.append(vec_words)\n",
    "            labels.append(i>0)\n",
    "            \n",
    "            big_word_count = max(big_word_count, len(text))\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# same dimensions\n",
    "for i, val in enumerate(data):\n",
    "    need = big_word_count - len(val)\n",
    "    data[i].extend(np.array([[1.0]*100]*need, dtype=np.float32))\n",
    "\n",
    "# shuffle\n",
    "import random\n",
    "tmp = list(zip(data, labels))\n",
    "random.shuffle(tmp)\n",
    "data, labels = zip(*tmp)\n",
    "\n",
    "# to numpy array\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 12\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "nr_samples_train = math.ceil(0.8*len(data))\n",
    "batch_size = 12\n",
    "print(nr_samples_train, batch_size)\n",
    "ds_train = tf.data.Dataset\\\n",
    "        .from_tensor_slices((data[:nr_samples_train], labels[:nr_samples_train]))\\\n",
    "        .batch(batch_size)\n",
    "ds_test = tf.data.Dataset\\\n",
    "        .from_tensor_slices((data[nr_samples_train:], labels[nr_samples_train:]))\\\n",
    "        .batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(p=2),semantics=\"forall\")\n",
    "Exists = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMean(p=2),semantics=\"exists\")\n",
    "formula_aggregator = ltn.Wrapper_Formula_Aggregator(ltn.fuzzy_ops.Aggreg_pMeanError(p=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "Valid = ltn.Predicate.MLP([(big_word_count, 100)],hidden_layer_sizes=(16,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def axioms(features, labels):\n",
    "    Valid_ex = ltn.Variable(\"Valid_ex\", features[labels])\n",
    "    Invalid_ex  = ltn.Variable(\"Invalid_ex\", features[tf.logical_not(labels)])\n",
    "    axioms = [\n",
    "        Forall(Valid_ex, Valid(Valid_ex)),\n",
    "        Forall(Invalid_ex, Not(Valid(Invalid_ex)))\n",
    "    ]\n",
    "    sat_level = formula_aggregator(axioms).tensor\n",
    "    return sat_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_metrics = tf.keras.metrics.Mean()\n",
    "\n",
    "trainable_variables = Valid.trainable_variables\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "for epoch in range(2000):\n",
    "    for _data, _labels in ds_train:\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = 1. - axioms(_data, _labels)\n",
    "        grads = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "    if epoch%100 == 0:\n",
    "        mean_metrics.reset_states()\n",
    "        for _data, _labels in ds_test:\n",
    "            mean_metrics(axioms(_data, _labels))\n",
    "        print(\"Epoch %d: Sat Level %.3f\"%(epoch, mean_metrics.result() ))\n",
    "mean_metrics.reset_states()\n",
    "for _data, _labels in ds_test:\n",
    "    mean_metrics(axioms(_data, _labels))\n",
    "print(\"Training finished at Epoch %d with Sat Level %.3f\"%(epoch, mean_metrics.result() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
