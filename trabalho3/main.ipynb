{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1) # pros resultados serem consistentes\n",
    "\n",
    "import logging; logging.basicConfig(level=logging.INFO)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logictensornetworks as ltn\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.linewidth'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package mac_morpho to\n",
      "[nltk_data]     /home/castro/nltk_data...\n",
      "[nltk_data]   Package mac_morpho is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('mac_morpho')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Word2Vec object from words.embedding\n",
      "INFO:gensim.utils:loading wv recursively from words.embedding.wv.* with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute cum_table to None\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname': 'words.embedding', 'datetime': '2021-12-03T21:21:40.130206', 'gensim': '4.1.2', 'python': '3.8.10 (default, Sep 28 2021, 16:10:42) \\n[GCC 9.3.0]', 'platform': 'Linux-5.10.60.1-microsoft-standard-WSL2-x86_64-with-glibc2.29', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "from nltk.corpus import mac_morpho\n",
    "import glob\n",
    "path = ['./false_text/','./true_text/']\n",
    "\n",
    "model = None\n",
    "if(not os.path.isfile('words.embedding')) :\n",
    "    model = gensim.models.Word2Vec(mac_morpho.sents())\n",
    "    new_texts = []\n",
    "    for i in range(2):\n",
    "        for file_name in glob.glob(path[i]+\"*.txt\"):\n",
    "            with open(file_name, 'r') as file:  \n",
    "                text = list(gensim.utils.tokenize(file.read()))\n",
    "                new_texts.append(text)\n",
    "    model.min_count = 1\n",
    "    model.build_vocab(new_texts, update=True)\n",
    "    model.train(new_texts, total_examples=len(new_texts), epochs=model.epochs)\n",
    "    \n",
    "    model.save('words.embedding')\n",
    "else:\n",
    "    model = gensim.models.Word2Vec.load('words.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "qtd_words = 10\n",
    "null_word = [0]*100\n",
    "big_word_count = 0\n",
    "for i in range(2):\n",
    "    for file_name in glob.glob(path[i]+\"*.txt\"):\n",
    "        with open(file_name, 'r') as file:\n",
    "            text = list(gensim.utils.tokenize(file.read()))\n",
    "            vec_words = [model.wv[word] for word in text]\n",
    "            data.append(vec_words)\n",
    "            labels.append(i>0)\n",
    "            \n",
    "            big_word_count = max(big_word_count, len(text))\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# same dimensions\n",
    "for i, val in enumerate(data):\n",
    "    need = big_word_count - len(val)\n",
    "    data[i].extend(np.array([[1.0]*100]*need, dtype=np.float32))\n",
    "\n",
    "# shuffle\n",
    "tmp = list(zip(data, labels))\n",
    "random.shuffle(tmp)\n",
    "data, labels = zip(*tmp)\n",
    "\n",
    "# to numpy array\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 32\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "nr_samples_train = math.ceil(0.7*len(data))\n",
    "batch_size = 32\n",
    "print(nr_samples_train, batch_size)\n",
    "ds_train = tf.data.Dataset\\\n",
    "        .from_tensor_slices((data[:nr_samples_train], labels[:nr_samples_train]))\\\n",
    "        .batch(batch_size)\n",
    "ds_test = tf.data.Dataset\\\n",
    "        .from_tensor_slices((data[nr_samples_train:], labels[nr_samples_train:]))\\\n",
    "        .batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(p=2),semantics=\"forall\")\n",
    "Exists = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMean(p=2),semantics=\"exists\")\n",
    "formula_aggregator = ltn.Wrapper_Formula_Aggregator(ltn.fuzzy_ops.Aggreg_pMeanError(p=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "Valid = ltn.Predicate.MLP([(big_word_count, 100)],hidden_layer_sizes=(16,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def axioms(features, labels):\n",
    "    Valid_ex = ltn.Variable(\"Valid_ex\", features[labels])\n",
    "    Invalid_ex  = ltn.Variable(\"Invalid_ex\", features[tf.logical_not(labels)])\n",
    "    axioms = [\n",
    "        Forall(Valid_ex, Valid(Valid_ex)),\n",
    "        Forall(Invalid_ex, Not(Valid(Invalid_ex)))\n",
    "    ]\n",
    "    sat_level = formula_aggregator(axioms).tensor\n",
    "    return sat_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Sat Level Test 0.492\n",
      "Epoch 0: Sat Level Train 0.502\n",
      "Epoch 500: Sat Level Test 0.518\n",
      "Epoch 500: Sat Level Train 0.594\n",
      "Epoch 1000: Sat Level Test 0.526\n",
      "Epoch 1000: Sat Level Train 0.636\n",
      "Epoch 1500: Sat Level Test 0.531\n",
      "Epoch 1500: Sat Level Train 0.671\n",
      "Epoch 2000: Sat Level Test 0.534\n",
      "Epoch 2000: Sat Level Train 0.705\n",
      "Epoch 2500: Sat Level Test 0.535\n",
      "Epoch 2500: Sat Level Train 0.735\n",
      "Epoch 3000: Sat Level Test 0.539\n",
      "Epoch 3000: Sat Level Train 0.760\n",
      "Epoch 3500: Sat Level Test 0.542\n",
      "Epoch 3500: Sat Level Train 0.784\n",
      "Epoch 4000: Sat Level Test 0.543\n",
      "Epoch 4000: Sat Level Train 0.805\n",
      "Epoch 4500: Sat Level Test 0.542\n",
      "Epoch 4500: Sat Level Train 0.823\n",
      "Epoch 5000: Sat Level Test 0.542\n",
      "Epoch 5000: Sat Level Train 0.841\n",
      "Epoch 5500: Sat Level Test 0.541\n",
      "Epoch 5500: Sat Level Train 0.857\n",
      "Epoch 6000: Sat Level Test 0.541\n",
      "Epoch 6000: Sat Level Train 0.873\n",
      "Epoch 6500: Sat Level Test 0.540\n",
      "Epoch 6500: Sat Level Train 0.888\n",
      "Epoch 7000: Sat Level Test 0.538\n",
      "Epoch 7000: Sat Level Train 0.901\n",
      "Epoch 7500: Sat Level Test 0.536\n",
      "Epoch 7500: Sat Level Train 0.914\n",
      "Epoch 8000: Sat Level Test 0.534\n",
      "Epoch 8000: Sat Level Train 0.925\n",
      "Epoch 8500: Sat Level Test 0.531\n",
      "Epoch 8500: Sat Level Train 0.935\n",
      "Epoch 9000: Sat Level Test 0.527\n",
      "Epoch 9000: Sat Level Train 0.944\n",
      "Epoch 9500: Sat Level Test 0.524\n",
      "Epoch 9500: Sat Level Train 0.952\n",
      "Training finished at Epoch 9999 with Sat Level 0.521\n"
     ]
    }
   ],
   "source": [
    "mean_metrics = tf.keras.metrics.Mean()\n",
    "\n",
    "trainable_variables = Valid.trainable_variables\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0000005)\n",
    "test_metrics = {}\n",
    "train_metrics = {}\n",
    "for epoch in range(10000):\n",
    "    for _data, _labels in ds_train:\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = 1. - axioms(_data, _labels)\n",
    "        grads = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "    if epoch%500 == 0:\n",
    "        mean_metrics.reset_states()\n",
    "        for _data, _labels in ds_test:\n",
    "            mean_metrics(axioms(_data, _labels))\n",
    "\n",
    "        test_metrics[epoch] = mean_metrics.result()\n",
    "        print(\"Epoch %d: Sat Level Test %.3f\"%(epoch, mean_metrics.result() ))\n",
    "\n",
    "        mean_metrics.reset_states()\n",
    "        for _data, _labels in ds_train:\n",
    "            mean_metrics(axioms(_data, _labels))\n",
    "            \n",
    "        train_metrics[epoch] = mean_metrics.result()\n",
    "        print(\"Epoch %d: Sat Level Train %.3f\"%(epoch, mean_metrics.result() ))   \n",
    "\n",
    "mean_metrics.reset_states()\n",
    "for _data, _labels in ds_test:\n",
    "    mean_metrics(axioms(_data, _labels))\n",
    "print(\"Training finished at Epoch %d with Sat Level %.3f\"%(epoch, mean_metrics.result() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: <tf.Tensor: shape=(), dtype=float32, numpy=0.5020493>, 500: <tf.Tensor: shape=(), dtype=float32, numpy=0.59402853>, 1000: <tf.Tensor: shape=(), dtype=float32, numpy=0.6355082>, 1500: <tf.Tensor: shape=(), dtype=float32, numpy=0.67119044>, 2000: <tf.Tensor: shape=(), dtype=float32, numpy=0.70526916>, 2500: <tf.Tensor: shape=(), dtype=float32, numpy=0.7345576>, 3000: <tf.Tensor: shape=(), dtype=float32, numpy=0.76046085>, 3500: <tf.Tensor: shape=(), dtype=float32, numpy=0.7839164>, 4000: <tf.Tensor: shape=(), dtype=float32, numpy=0.8046182>, 4500: <tf.Tensor: shape=(), dtype=float32, numpy=0.8233612>, 5000: <tf.Tensor: shape=(), dtype=float32, numpy=0.84083456>, 5500: <tf.Tensor: shape=(), dtype=float32, numpy=0.8573732>, 6000: <tf.Tensor: shape=(), dtype=float32, numpy=0.87300014>, 6500: <tf.Tensor: shape=(), dtype=float32, numpy=0.8878417>, 7000: <tf.Tensor: shape=(), dtype=float32, numpy=0.901402>, 7500: <tf.Tensor: shape=(), dtype=float32, numpy=0.9137239>, 8000: <tf.Tensor: shape=(), dtype=float32, numpy=0.92490953>, 8500: <tf.Tensor: shape=(), dtype=float32, numpy=0.93496865>, 9000: <tf.Tensor: shape=(), dtype=float32, numpy=0.9438644>, 9500: <tf.Tensor: shape=(), dtype=float32, numpy=0.95165306>}\n",
      "{0: <tf.Tensor: shape=(), dtype=float32, numpy=0.4916181>, 500: <tf.Tensor: shape=(), dtype=float32, numpy=0.51845694>, 1000: <tf.Tensor: shape=(), dtype=float32, numpy=0.5263237>, 1500: <tf.Tensor: shape=(), dtype=float32, numpy=0.5312499>, 2000: <tf.Tensor: shape=(), dtype=float32, numpy=0.53371406>, 2500: <tf.Tensor: shape=(), dtype=float32, numpy=0.53545225>, 3000: <tf.Tensor: shape=(), dtype=float32, numpy=0.5386962>, 3500: <tf.Tensor: shape=(), dtype=float32, numpy=0.5419353>, 4000: <tf.Tensor: shape=(), dtype=float32, numpy=0.542558>, 4500: <tf.Tensor: shape=(), dtype=float32, numpy=0.5423898>, 5000: <tf.Tensor: shape=(), dtype=float32, numpy=0.5419992>, 5500: <tf.Tensor: shape=(), dtype=float32, numpy=0.541152>, 6000: <tf.Tensor: shape=(), dtype=float32, numpy=0.5408257>, 6500: <tf.Tensor: shape=(), dtype=float32, numpy=0.5395001>, 7000: <tf.Tensor: shape=(), dtype=float32, numpy=0.53799427>, 7500: <tf.Tensor: shape=(), dtype=float32, numpy=0.5362165>, 8000: <tf.Tensor: shape=(), dtype=float32, numpy=0.53355765>, 8500: <tf.Tensor: shape=(), dtype=float32, numpy=0.5305202>, 9000: <tf.Tensor: shape=(), dtype=float32, numpy=0.5272589>, 9500: <tf.Tensor: shape=(), dtype=float32, numpy=0.5240037>}\n"
     ]
    }
   ],
   "source": [
    "print(train_metrics)\n",
    "print(test_metrics)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
